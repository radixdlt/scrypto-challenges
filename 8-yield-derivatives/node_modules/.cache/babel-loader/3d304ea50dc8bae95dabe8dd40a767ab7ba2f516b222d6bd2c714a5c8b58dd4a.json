{"ast":null,"code":"// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\nimport { APIResource } from 'openai/resource';\nexport class Completions extends APIResource {\n  create(body, options) {\n    return this._client.post('/chat/completions', {\n      body,\n      ...options,\n      stream: body.stream ?? false\n    });\n  }\n}\n(function (Completions) {})(Completions || (Completions = {}));","map":{"version":3,"names":["APIResource","Completions","create","body","options","_client","post","stream"],"sources":["/Users/mdabdurrazzak/RadInsu/node_modules/openai/src/resources/chat/completions.ts"],"sourcesContent":["// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport * as Core from \"../../core\";\nimport { APIPromise } from \"../../core\";\nimport { APIResource } from \"../../resource\";\nimport * as ChatCompletionsAPI from \"./completions\";\nimport * as CompletionsAPI from \"../completions\";\nimport * as Shared from \"../shared\";\nimport * as ChatAPI from \"./chat\";\nimport { Stream } from \"../../streaming\";\n\nexport class Completions extends APIResource {\n  /**\n   * Creates a model response for the given chat conversation.\n   */\n  create(\n    body: ChatCompletionCreateParamsNonStreaming,\n    options?: Core.RequestOptions,\n  ): APIPromise<ChatCompletion>;\n  create(\n    body: ChatCompletionCreateParamsStreaming,\n    options?: Core.RequestOptions,\n  ): APIPromise<Stream<ChatCompletionChunk>>;\n  create(\n    body: ChatCompletionCreateParamsBase,\n    options?: Core.RequestOptions,\n  ): APIPromise<Stream<ChatCompletionChunk> | ChatCompletion>;\n  create(\n    body: ChatCompletionCreateParams,\n    options?: Core.RequestOptions,\n  ): APIPromise<ChatCompletion> | APIPromise<Stream<ChatCompletionChunk>> {\n    return this._client.post('/chat/completions', { body, ...options, stream: body.stream ?? false }) as\n      | APIPromise<ChatCompletion>\n      | APIPromise<Stream<ChatCompletionChunk>>;\n  }\n}\n\n/**\n * Represents a chat completion response returned by model, based on the provided\n * input.\n */\nexport interface ChatCompletion {\n  /**\n   * A unique identifier for the chat completion.\n   */\n  id: string;\n\n  /**\n   * A list of chat completion choices. Can be more than one if `n` is greater\n   * than 1.\n   */\n  choices: Array<ChatCompletion.Choice>;\n\n  /**\n   * The Unix timestamp (in seconds) of when the chat completion was created.\n   */\n  created: number;\n\n  /**\n   * The model used for the chat completion.\n   */\n  model: string;\n\n  /**\n   * The object type, which is always `chat.completion`.\n   */\n  object: 'chat.completion';\n\n  /**\n   * This fingerprint represents the backend configuration that the model runs with.\n   *\n   * Can be used in conjunction with the `seed` request parameter to understand when\n   * backend changes have been made that might impact determinism.\n   */\n  system_fingerprint?: string;\n\n  /**\n   * Usage statistics for the completion request.\n   */\n  usage?: CompletionsAPI.CompletionUsage;\n}\n\nexport namespace ChatCompletion {\n  export interface Choice {\n    /**\n     * The reason the model stopped generating tokens. This will be `stop` if the model\n     * hit a natural stop point or a provided stop sequence, `length` if the maximum\n     * number of tokens specified in the request was reached, `content_filter` if\n     * content was omitted due to a flag from our content filters, `tool_calls` if the\n     * model called a tool, or `function_call` (deprecated) if the model called a\n     * function.\n     */\n    finish_reason: 'stop' | 'length' | 'tool_calls' | 'content_filter' | 'function_call';\n\n    /**\n     * The index of the choice in the list of choices.\n     */\n    index: number;\n\n    /**\n     * Log probability information for the choice.\n     */\n    logprobs: Choice.Logprobs | null;\n\n    /**\n     * A chat completion message generated by the model.\n     */\n    message: ChatCompletionsAPI.ChatCompletionMessage;\n  }\n\n  export namespace Choice {\n    /**\n     * Log probability information for the choice.\n     */\n    export interface Logprobs {\n      /**\n       * A list of message content tokens with log probability information.\n       */\n      content: Array<ChatCompletionsAPI.ChatCompletionTokenLogprob> | null;\n    }\n  }\n}\n\nexport interface ChatCompletionAssistantMessageParam {\n  /**\n   * The role of the messages author, in this case `assistant`.\n   */\n  role: 'assistant';\n\n  /**\n   * The contents of the assistant message. Required unless `tool_calls` or\n   * `function_call` is specified.\n   */\n  content?: string | null;\n\n  /**\n   * @deprecated: Deprecated and replaced by `tool_calls`. The name and arguments of\n   * a function that should be called, as generated by the model.\n   */\n  function_call?: ChatCompletionAssistantMessageParam.FunctionCall;\n\n  /**\n   * An optional name for the participant. Provides the model information to\n   * differentiate between participants of the same role.\n   */\n  name?: string;\n\n  /**\n   * The tool calls generated by the model, such as function calls.\n   */\n  tool_calls?: Array<ChatCompletionMessageToolCall>;\n}\n\nexport namespace ChatCompletionAssistantMessageParam {\n  /**\n   * @deprecated: Deprecated and replaced by `tool_calls`. The name and arguments of\n   * a function that should be called, as generated by the model.\n   */\n  export interface FunctionCall {\n    /**\n     * The arguments to call the function with, as generated by the model in JSON\n     * format. Note that the model does not always generate valid JSON, and may\n     * hallucinate parameters not defined by your function schema. Validate the\n     * arguments in your code before calling your function.\n     */\n    arguments: string;\n\n    /**\n     * The name of the function to call.\n     */\n    name: string;\n  }\n}\n\n/**\n * Represents a streamed chunk of a chat completion response returned by model,\n * based on the provided input.\n */\nexport interface ChatCompletionChunk {\n  /**\n   * A unique identifier for the chat completion. Each chunk has the same ID.\n   */\n  id: string;\n\n  /**\n   * A list of chat completion choices. Can be more than one if `n` is greater\n   * than 1.\n   */\n  choices: Array<ChatCompletionChunk.Choice>;\n\n  /**\n   * The Unix timestamp (in seconds) of when the chat completion was created. Each\n   * chunk has the same timestamp.\n   */\n  created: number;\n\n  /**\n   * The model to generate the completion.\n   */\n  model: string;\n\n  /**\n   * The object type, which is always `chat.completion.chunk`.\n   */\n  object: 'chat.completion.chunk';\n\n  /**\n   * This fingerprint represents the backend configuration that the model runs with.\n   * Can be used in conjunction with the `seed` request parameter to understand when\n   * backend changes have been made that might impact determinism.\n   */\n  system_fingerprint?: string;\n}\n\nexport namespace ChatCompletionChunk {\n  export interface Choice {\n    /**\n     * A chat completion delta generated by streamed model responses.\n     */\n    delta: Choice.Delta;\n\n    /**\n     * The reason the model stopped generating tokens. This will be `stop` if the model\n     * hit a natural stop point or a provided stop sequence, `length` if the maximum\n     * number of tokens specified in the request was reached, `content_filter` if\n     * content was omitted due to a flag from our content filters, `tool_calls` if the\n     * model called a tool, or `function_call` (deprecated) if the model called a\n     * function.\n     */\n    finish_reason: 'stop' | 'length' | 'tool_calls' | 'content_filter' | 'function_call' | null;\n\n    /**\n     * The index of the choice in the list of choices.\n     */\n    index: number;\n\n    /**\n     * Log probability information for the choice.\n     */\n    logprobs?: Choice.Logprobs | null;\n  }\n\n  export namespace Choice {\n    /**\n     * A chat completion delta generated by streamed model responses.\n     */\n    export interface Delta {\n      /**\n       * The contents of the chunk message.\n       */\n      content?: string | null;\n\n      /**\n       * @deprecated: Deprecated and replaced by `tool_calls`. The name and arguments of\n       * a function that should be called, as generated by the model.\n       */\n      function_call?: Delta.FunctionCall;\n\n      /**\n       * The role of the author of this message.\n       */\n      role?: 'system' | 'user' | 'assistant' | 'tool';\n\n      tool_calls?: Array<Delta.ToolCall>;\n    }\n\n    export namespace Delta {\n      /**\n       * @deprecated: Deprecated and replaced by `tool_calls`. The name and arguments of\n       * a function that should be called, as generated by the model.\n       */\n      export interface FunctionCall {\n        /**\n         * The arguments to call the function with, as generated by the model in JSON\n         * format. Note that the model does not always generate valid JSON, and may\n         * hallucinate parameters not defined by your function schema. Validate the\n         * arguments in your code before calling your function.\n         */\n        arguments?: string;\n\n        /**\n         * The name of the function to call.\n         */\n        name?: string;\n      }\n\n      export interface ToolCall {\n        index: number;\n\n        /**\n         * The ID of the tool call.\n         */\n        id?: string;\n\n        function?: ToolCall.Function;\n\n        /**\n         * The type of the tool. Currently, only `function` is supported.\n         */\n        type?: 'function';\n      }\n\n      export namespace ToolCall {\n        export interface Function {\n          /**\n           * The arguments to call the function with, as generated by the model in JSON\n           * format. Note that the model does not always generate valid JSON, and may\n           * hallucinate parameters not defined by your function schema. Validate the\n           * arguments in your code before calling your function.\n           */\n          arguments?: string;\n\n          /**\n           * The name of the function to call.\n           */\n          name?: string;\n        }\n      }\n    }\n\n    /**\n     * Log probability information for the choice.\n     */\n    export interface Logprobs {\n      /**\n       * A list of message content tokens with log probability information.\n       */\n      content: Array<ChatCompletionsAPI.ChatCompletionTokenLogprob> | null;\n    }\n  }\n}\n\nexport type ChatCompletionContentPart = ChatCompletionContentPartText | ChatCompletionContentPartImage;\n\nexport interface ChatCompletionContentPartImage {\n  image_url: ChatCompletionContentPartImage.ImageURL;\n\n  /**\n   * The type of the content part.\n   */\n  type: 'image_url';\n}\n\nexport namespace ChatCompletionContentPartImage {\n  export interface ImageURL {\n    /**\n     * Either a URL of the image or the base64 encoded image data.\n     */\n    url: string;\n\n    /**\n     * Specifies the detail level of the image. Learn more in the\n     * [Vision guide](https://platform.openai.com/docs/guides/vision/low-or-high-fidelity-image-understanding).\n     */\n    detail?: 'auto' | 'low' | 'high';\n  }\n}\n\nexport interface ChatCompletionContentPartText {\n  /**\n   * The text content.\n   */\n  text: string;\n\n  /**\n   * The type of the content part.\n   */\n  type: 'text';\n}\n\n/**\n * Specifying a particular function via `{\"name\": \"my_function\"}` forces the model\n * to call that function.\n */\nexport interface ChatCompletionFunctionCallOption {\n  /**\n   * The name of the function to call.\n   */\n  name: string;\n}\n\n/**\n * @deprecated\n */\nexport interface ChatCompletionFunctionMessageParam {\n  /**\n   * The contents of the function message.\n   */\n  content: string | null;\n\n  /**\n   * The name of the function to call.\n   */\n  name: string;\n\n  /**\n   * The role of the messages author, in this case `function`.\n   */\n  role: 'function';\n}\n\n/**\n * A chat completion message generated by the model.\n */\nexport interface ChatCompletionMessage {\n  /**\n   * The contents of the message.\n   */\n  content: string | null;\n\n  /**\n   * The role of the author of this message.\n   */\n  role: 'assistant';\n\n  /**\n   * @deprecated: Deprecated and replaced by `tool_calls`. The name and arguments of\n   * a function that should be called, as generated by the model.\n   */\n  function_call?: ChatCompletionMessage.FunctionCall;\n\n  /**\n   * The tool calls generated by the model, such as function calls.\n   */\n  tool_calls?: Array<ChatCompletionMessageToolCall>;\n}\n\nexport namespace ChatCompletionMessage {\n  /**\n   * @deprecated: Deprecated and replaced by `tool_calls`. The name and arguments of\n   * a function that should be called, as generated by the model.\n   */\n  export interface FunctionCall {\n    /**\n     * The arguments to call the function with, as generated by the model in JSON\n     * format. Note that the model does not always generate valid JSON, and may\n     * hallucinate parameters not defined by your function schema. Validate the\n     * arguments in your code before calling your function.\n     */\n    arguments: string;\n\n    /**\n     * The name of the function to call.\n     */\n    name: string;\n  }\n}\n\nexport type ChatCompletionMessageParam =\n  | ChatCompletionSystemMessageParam\n  | ChatCompletionUserMessageParam\n  | ChatCompletionAssistantMessageParam\n  | ChatCompletionToolMessageParam\n  | ChatCompletionFunctionMessageParam;\n\nexport interface ChatCompletionMessageToolCall {\n  /**\n   * The ID of the tool call.\n   */\n  id: string;\n\n  /**\n   * The function that the model called.\n   */\n  function: ChatCompletionMessageToolCall.Function;\n\n  /**\n   * The type of the tool. Currently, only `function` is supported.\n   */\n  type: 'function';\n}\n\nexport namespace ChatCompletionMessageToolCall {\n  /**\n   * The function that the model called.\n   */\n  export interface Function {\n    /**\n     * The arguments to call the function with, as generated by the model in JSON\n     * format. Note that the model does not always generate valid JSON, and may\n     * hallucinate parameters not defined by your function schema. Validate the\n     * arguments in your code before calling your function.\n     */\n    arguments: string;\n\n    /**\n     * The name of the function to call.\n     */\n    name: string;\n  }\n}\n\n/**\n * Specifies a tool the model should use. Use to force the model to call a specific\n * function.\n */\nexport interface ChatCompletionNamedToolChoice {\n  function: ChatCompletionNamedToolChoice.Function;\n\n  /**\n   * The type of the tool. Currently, only `function` is supported.\n   */\n  type: 'function';\n}\n\nexport namespace ChatCompletionNamedToolChoice {\n  export interface Function {\n    /**\n     * The name of the function to call.\n     */\n    name: string;\n  }\n}\n\n/**\n * The role of the author of a message\n */\nexport type ChatCompletionRole = 'system' | 'user' | 'assistant' | 'tool' | 'function';\n\nexport interface ChatCompletionSystemMessageParam {\n  /**\n   * The contents of the system message.\n   */\n  content: string;\n\n  /**\n   * The role of the messages author, in this case `system`.\n   */\n  role: 'system';\n\n  /**\n   * An optional name for the participant. Provides the model information to\n   * differentiate between participants of the same role.\n   */\n  name?: string;\n}\n\nexport interface ChatCompletionTokenLogprob {\n  /**\n   * The token.\n   */\n  token: string;\n\n  /**\n   * A list of integers representing the UTF-8 bytes representation of the token.\n   * Useful in instances where characters are represented by multiple tokens and\n   * their byte representations must be combined to generate the correct text\n   * representation. Can be `null` if there is no bytes representation for the token.\n   */\n  bytes: Array<number> | null;\n\n  /**\n   * The log probability of this token, if it is within the top 20 most likely\n   * tokens. Otherwise, the value `-9999.0` is used to signify that the token is very\n   * unlikely.\n   */\n  logprob: number;\n\n  /**\n   * List of the most likely tokens and their log probability, at this token\n   * position. In rare cases, there may be fewer than the number of requested\n   * `top_logprobs` returned.\n   */\n  top_logprobs: Array<ChatCompletionTokenLogprob.TopLogprob>;\n}\n\nexport namespace ChatCompletionTokenLogprob {\n  export interface TopLogprob {\n    /**\n     * The token.\n     */\n    token: string;\n\n    /**\n     * A list of integers representing the UTF-8 bytes representation of the token.\n     * Useful in instances where characters are represented by multiple tokens and\n     * their byte representations must be combined to generate the correct text\n     * representation. Can be `null` if there is no bytes representation for the token.\n     */\n    bytes: Array<number> | null;\n\n    /**\n     * The log probability of this token, if it is within the top 20 most likely\n     * tokens. Otherwise, the value `-9999.0` is used to signify that the token is very\n     * unlikely.\n     */\n    logprob: number;\n  }\n}\n\nexport interface ChatCompletionTool {\n  function: Shared.FunctionDefinition;\n\n  /**\n   * The type of the tool. Currently, only `function` is supported.\n   */\n  type: 'function';\n}\n\n/**\n * Controls which (if any) function is called by the model. `none` means the model\n * will not call a function and instead generates a message. `auto` means the model\n * can pick between generating a message or calling a function. Specifying a\n * particular function via\n * `{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}` forces the model to\n * call that function.\n *\n * `none` is the default when no functions are present. `auto` is the default if\n * functions are present.\n */\nexport type ChatCompletionToolChoiceOption = 'none' | 'auto' | ChatCompletionNamedToolChoice;\n\nexport interface ChatCompletionToolMessageParam {\n  /**\n   * The contents of the tool message.\n   */\n  content: string;\n\n  /**\n   * The role of the messages author, in this case `tool`.\n   */\n  role: 'tool';\n\n  /**\n   * Tool call that this message is responding to.\n   */\n  tool_call_id: string;\n}\n\nexport interface ChatCompletionUserMessageParam {\n  /**\n   * The contents of the user message.\n   */\n  content: string | Array<ChatCompletionContentPart>;\n\n  /**\n   * The role of the messages author, in this case `user`.\n   */\n  role: 'user';\n\n  /**\n   * An optional name for the participant. Provides the model information to\n   * differentiate between participants of the same role.\n   */\n  name?: string;\n}\n\n/**\n * @deprecated ChatCompletionMessageParam should be used instead\n */\nexport type CreateChatCompletionRequestMessage = ChatCompletionMessageParam;\n\nexport type ChatCompletionCreateParams =\n  | ChatCompletionCreateParamsNonStreaming\n  | ChatCompletionCreateParamsStreaming;\n\nexport interface ChatCompletionCreateParamsBase {\n  /**\n   * A list of messages comprising the conversation so far.\n   * [Example Python code](https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models).\n   */\n  messages: Array<ChatCompletionMessageParam>;\n\n  /**\n   * ID of the model to use. See the\n   * [model endpoint compatibility](https://platform.openai.com/docs/models/model-endpoint-compatibility)\n   * table for details on which models work with the Chat API.\n   */\n  model: (string & {}) | ChatAPI.ChatModel;\n\n  /**\n   * Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n   * existing frequency in the text so far, decreasing the model's likelihood to\n   * repeat the same line verbatim.\n   *\n   * [See more information about frequency and presence penalties.](https://platform.openai.com/docs/guides/text-generation/parameter-details)\n   */\n  frequency_penalty?: number | null;\n\n  /**\n   * Deprecated in favor of `tool_choice`.\n   *\n   * Controls which (if any) function is called by the model. `none` means the model\n   * will not call a function and instead generates a message. `auto` means the model\n   * can pick between generating a message or calling a function. Specifying a\n   * particular function via `{\"name\": \"my_function\"}` forces the model to call that\n   * function.\n   *\n   * `none` is the default when no functions are present. `auto` is the default if\n   * functions are present.\n   */\n  function_call?: 'none' | 'auto' | ChatCompletionFunctionCallOption;\n\n  /**\n   * Deprecated in favor of `tools`.\n   *\n   * A list of functions the model may generate JSON inputs for.\n   */\n  functions?: Array<ChatCompletionCreateParams.Function>;\n\n  /**\n   * Modify the likelihood of specified tokens appearing in the completion.\n   *\n   * Accepts a JSON object that maps tokens (specified by their token ID in the\n   * tokenizer) to an associated bias value from -100 to 100. Mathematically, the\n   * bias is added to the logits generated by the model prior to sampling. The exact\n   * effect will vary per model, but values between -1 and 1 should decrease or\n   * increase likelihood of selection; values like -100 or 100 should result in a ban\n   * or exclusive selection of the relevant token.\n   */\n  logit_bias?: Record<string, number> | null;\n\n  /**\n   * Whether to return log probabilities of the output tokens or not. If true,\n   * returns the log probabilities of each output token returned in the `content` of\n   * `message`.\n   */\n  logprobs?: boolean | null;\n\n  /**\n   * The maximum number of [tokens](/tokenizer) that can be generated in the chat\n   * completion.\n   *\n   * The total length of input tokens and generated tokens is limited by the model's\n   * context length.\n   * [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken)\n   * for counting tokens.\n   */\n  max_tokens?: number | null;\n\n  /**\n   * How many chat completion choices to generate for each input message. Note that\n   * you will be charged based on the number of generated tokens across all of the\n   * choices. Keep `n` as `1` to minimize costs.\n   */\n  n?: number | null;\n\n  /**\n   * Number between -2.0 and 2.0. Positive values penalize new tokens based on\n   * whether they appear in the text so far, increasing the model's likelihood to\n   * talk about new topics.\n   *\n   * [See more information about frequency and presence penalties.](https://platform.openai.com/docs/guides/text-generation/parameter-details)\n   */\n  presence_penalty?: number | null;\n\n  /**\n   * An object specifying the format that the model must output. Compatible with\n   * [GPT-4 Turbo](https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo) and\n   * all GPT-3.5 Turbo models newer than `gpt-3.5-turbo-1106`.\n   *\n   * Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which guarantees the\n   * message the model generates is valid JSON.\n   *\n   * **Important:** when using JSON mode, you **must** also instruct the model to\n   * produce JSON yourself via a system or user message. Without this, the model may\n   * generate an unending stream of whitespace until the generation reaches the token\n   * limit, resulting in a long-running and seemingly \"stuck\" request. Also note that\n   * the message content may be partially cut off if `finish_reason=\"length\"`, which\n   * indicates the generation exceeded `max_tokens` or the conversation exceeded the\n   * max context length.\n   */\n  response_format?: ChatCompletionCreateParams.ResponseFormat;\n\n  /**\n   * This feature is in Beta. If specified, our system will make a best effort to\n   * sample deterministically, such that repeated requests with the same `seed` and\n   * parameters should return the same result. Determinism is not guaranteed, and you\n   * should refer to the `system_fingerprint` response parameter to monitor changes\n   * in the backend.\n   */\n  seed?: number | null;\n\n  /**\n   * Up to 4 sequences where the API will stop generating further tokens.\n   */\n  stop?: string | null | Array<string>;\n\n  /**\n   * If set, partial message deltas will be sent, like in ChatGPT. Tokens will be\n   * sent as data-only\n   * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)\n   * as they become available, with the stream terminated by a `data: [DONE]`\n   * message.\n   * [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).\n   */\n  stream?: boolean | null;\n\n  /**\n   * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n   * make the output more random, while lower values like 0.2 will make it more\n   * focused and deterministic.\n   *\n   * We generally recommend altering this or `top_p` but not both.\n   */\n  temperature?: number | null;\n\n  /**\n   * Controls which (if any) function is called by the model. `none` means the model\n   * will not call a function and instead generates a message. `auto` means the model\n   * can pick between generating a message or calling a function. Specifying a\n   * particular function via\n   * `{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}` forces the model to\n   * call that function.\n   *\n   * `none` is the default when no functions are present. `auto` is the default if\n   * functions are present.\n   */\n  tool_choice?: ChatCompletionToolChoiceOption;\n\n  /**\n   * A list of tools the model may call. Currently, only functions are supported as a\n   * tool. Use this to provide a list of functions the model may generate JSON inputs\n   * for. A max of 128 functions are supported.\n   */\n  tools?: Array<ChatCompletionTool>;\n\n  /**\n   * An integer between 0 and 20 specifying the number of most likely tokens to\n   * return at each token position, each with an associated log probability.\n   * `logprobs` must be set to `true` if this parameter is used.\n   */\n  top_logprobs?: number | null;\n\n  /**\n   * An alternative to sampling with temperature, called nucleus sampling, where the\n   * model considers the results of the tokens with top_p probability mass. So 0.1\n   * means only the tokens comprising the top 10% probability mass are considered.\n   *\n   * We generally recommend altering this or `temperature` but not both.\n   */\n  top_p?: number | null;\n\n  /**\n   * A unique identifier representing your end-user, which can help OpenAI to monitor\n   * and detect abuse.\n   * [Learn more](https://platform.openai.com/docs/guides/safety-best-practices/end-user-ids).\n   */\n  user?: string;\n}\n\nexport namespace ChatCompletionCreateParams {\n  /**\n   * @deprecated\n   */\n  export interface Function {\n    /**\n     * The name of the function to be called. Must be a-z, A-Z, 0-9, or contain\n     * underscores and dashes, with a maximum length of 64.\n     */\n    name: string;\n\n    /**\n     * A description of what the function does, used by the model to choose when and\n     * how to call the function.\n     */\n    description?: string;\n\n    /**\n     * The parameters the functions accepts, described as a JSON Schema object. See the\n     * [guide](https://platform.openai.com/docs/guides/text-generation/function-calling)\n     * for examples, and the\n     * [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for\n     * documentation about the format.\n     *\n     * Omitting `parameters` defines a function with an empty parameter list.\n     */\n    parameters?: Shared.FunctionParameters;\n  }\n\n  /**\n   * An object specifying the format that the model must output. Compatible with\n   * [GPT-4 Turbo](https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo) and\n   * all GPT-3.5 Turbo models newer than `gpt-3.5-turbo-1106`.\n   *\n   * Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which guarantees the\n   * message the model generates is valid JSON.\n   *\n   * **Important:** when using JSON mode, you **must** also instruct the model to\n   * produce JSON yourself via a system or user message. Without this, the model may\n   * generate an unending stream of whitespace until the generation reaches the token\n   * limit, resulting in a long-running and seemingly \"stuck\" request. Also note that\n   * the message content may be partially cut off if `finish_reason=\"length\"`, which\n   * indicates the generation exceeded `max_tokens` or the conversation exceeded the\n   * max context length.\n   */\n  export interface ResponseFormat {\n    /**\n     * Must be one of `text` or `json_object`.\n     */\n    type?: 'text' | 'json_object';\n  }\n\n  export type ChatCompletionCreateParamsNonStreaming =\n    ChatCompletionsAPI.ChatCompletionCreateParamsNonStreaming;\n  export type ChatCompletionCreateParamsStreaming = ChatCompletionsAPI.ChatCompletionCreateParamsStreaming;\n}\n\n/**\n * @deprecated Use ChatCompletionCreateParams instead\n */\nexport type CompletionCreateParams = ChatCompletionCreateParams;\n\nexport interface ChatCompletionCreateParamsNonStreaming extends ChatCompletionCreateParamsBase {\n  /**\n   * If set, partial message deltas will be sent, like in ChatGPT. Tokens will be\n   * sent as data-only\n   * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)\n   * as they become available, with the stream terminated by a `data: [DONE]`\n   * message.\n   * [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).\n   */\n  stream?: false | null;\n}\n\n/**\n * @deprecated Use ChatCompletionCreateParamsNonStreaming instead\n */\nexport type CompletionCreateParamsNonStreaming = ChatCompletionCreateParamsNonStreaming;\n\nexport interface ChatCompletionCreateParamsStreaming extends ChatCompletionCreateParamsBase {\n  /**\n   * If set, partial message deltas will be sent, like in ChatGPT. Tokens will be\n   * sent as data-only\n   * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)\n   * as they become available, with the stream terminated by a `data: [DONE]`\n   * message.\n   * [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).\n   */\n  stream: true;\n}\n\n/**\n * @deprecated Use ChatCompletionCreateParamsStreaming instead\n */\nexport type CompletionCreateParamsStreaming = ChatCompletionCreateParamsStreaming;\n\nexport namespace Completions {\n  export import ChatCompletion = ChatCompletionsAPI.ChatCompletion;\n  export import ChatCompletionAssistantMessageParam = ChatCompletionsAPI.ChatCompletionAssistantMessageParam;\n  export import ChatCompletionChunk = ChatCompletionsAPI.ChatCompletionChunk;\n  export import ChatCompletionContentPart = ChatCompletionsAPI.ChatCompletionContentPart;\n  export import ChatCompletionContentPartImage = ChatCompletionsAPI.ChatCompletionContentPartImage;\n  export import ChatCompletionContentPartText = ChatCompletionsAPI.ChatCompletionContentPartText;\n  export import ChatCompletionFunctionCallOption = ChatCompletionsAPI.ChatCompletionFunctionCallOption;\n  export import ChatCompletionFunctionMessageParam = ChatCompletionsAPI.ChatCompletionFunctionMessageParam;\n  export import ChatCompletionMessage = ChatCompletionsAPI.ChatCompletionMessage;\n  export import ChatCompletionMessageParam = ChatCompletionsAPI.ChatCompletionMessageParam;\n  export import ChatCompletionMessageToolCall = ChatCompletionsAPI.ChatCompletionMessageToolCall;\n  export import ChatCompletionNamedToolChoice = ChatCompletionsAPI.ChatCompletionNamedToolChoice;\n  export import ChatCompletionRole = ChatCompletionsAPI.ChatCompletionRole;\n  export import ChatCompletionSystemMessageParam = ChatCompletionsAPI.ChatCompletionSystemMessageParam;\n  export import ChatCompletionTokenLogprob = ChatCompletionsAPI.ChatCompletionTokenLogprob;\n  export import ChatCompletionTool = ChatCompletionsAPI.ChatCompletionTool;\n  export import ChatCompletionToolChoiceOption = ChatCompletionsAPI.ChatCompletionToolChoiceOption;\n  export import ChatCompletionToolMessageParam = ChatCompletionsAPI.ChatCompletionToolMessageParam;\n  export import ChatCompletionUserMessageParam = ChatCompletionsAPI.ChatCompletionUserMessageParam;\n  /**\n   * @deprecated ChatCompletionMessageParam should be used instead\n   */\n  export import CreateChatCompletionRequestMessage = ChatCompletionsAPI.CreateChatCompletionRequestMessage;\n  export import ChatCompletionCreateParams = ChatCompletionsAPI.ChatCompletionCreateParams;\n  export import CompletionCreateParams = ChatCompletionsAPI.CompletionCreateParams;\n  export import ChatCompletionCreateParamsNonStreaming = ChatCompletionsAPI.ChatCompletionCreateParamsNonStreaming;\n  export import CompletionCreateParamsNonStreaming = ChatCompletionsAPI.CompletionCreateParamsNonStreaming;\n  export import ChatCompletionCreateParamsStreaming = ChatCompletionsAPI.ChatCompletionCreateParamsStreaming;\n  export import CompletionCreateParamsStreaming = ChatCompletionsAPI.CompletionCreateParamsStreaming;\n}\n"],"mappings":"AAAA;SAISA,WAAW,QAAQ,iBAAiB;AAO7C,OAAM,MAAOC,WAAY,SAAQD,WAAW;EAgB1CE,MAAMA,CACJC,IAAgC,EAChCC,OAA6B;IAE7B,OAAO,IAAI,CAACC,OAAO,CAACC,IAAI,CAAC,mBAAmB,EAAE;MAAEH,IAAI;MAAE,GAAGC,OAAO;MAAEG,MAAM,EAAEJ,IAAI,CAACI,MAAM,IAAI;IAAK,CAAE,CAErD;EAC7C;;AAu4BF,WAAiBN,WAAW,GA8B5B,CAAC,EA9BgBA,WAAW,KAAXA,WAAW","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}